\documentclass[10pt,twoneside]{article}
%\documentclass[10pt,oneside]{book}
%\documentclass[14pt]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
%\pagestyle{headings}

\usepackage{natbib}
\usepackage{hyperref}



\usepackage{titlesec}


\usepackage{enumitem}

%\usepackage[spanish]{babel}
 
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}

\usepackage{blindtext}
\usepackage[T1]{fontenc}


\usepackage{bbm}

\usepackage[margin=5ex,footskip=.25in]{geometry}


\usepackage{geometry}
\usepackage{mathtools}    
\usepackage[latin9]{inputenc}  	
\usepackage[bottom]{footmisc}	
\geometry{letterpaper}                   		
\usepackage{graphicx}														
\usepackage{amssymb}
\usepackage{ragged2e}
\newcommand{\kfour}{\mathbb{K}^4}
\newcommand{\ktwo}{\mathbb{K}^2}
\newcommand{\pthree}{\mathbb{P}^3}
\newcommand{\afin}{\mathbb{A}}
\newcommand{\ov}{\overrightarrow}


\usepackage{tikz-cd}


\usepackage{faktor}


\usepackage{mathtools}


%\usepackage[spanish]{babel} % espaÃƒÂ±ol
%\usepackage[utf8]{inputenc} % acentos sin codigo
%\usepackage{graphicx} % graficos
%\usepackage{amssymb} 
%\usepackage{amsmath,amsthm} 
%\usepackage{amsthm} 

%\usepackage{hyperref}
%\usepackage{mathtools}
\usepackage{mathrsfs} % Letras caligrÃ¡ficas
\usepackage{bm}



\usepackage{xcolor}

\newcommand{\mcm}{\qopname \relax o{mcm}}

\newcommand{\massey}[3]{\langle[ {#1} ],[ {#2} ],[ {#3} ]\rangle}
\newcommand{\module}[1]{\vert #1 \lvert }
\newcommand{\dotprod}[2]{\langle #1,#2 \rangle}
\newcommand{\mbot}{{(\bot)}}
%\newcommand{\isoequal}{\stackrel{\text{iso}}{=}}
\newcommand{\isoequal}{\simeq}
\newcommand{\difequal}{\stackrel{\text{dif}}{=}}
\newcommand{\defequal}{\stackrel{\text{def}}{=}}
\newcommand{\isoapprox}{\stackrel{\text{iso}}{\displaystyle\approx}}
\newcommand{\Cech}{\v{C}ech}
\newcommand{\difpartial}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\difantipartial}[2]{\frac{\partial #1}{\overline{\partial} #2}}
\newcommand{\antipartial}{{\overline{\partial }}}

\newcommand{\comilla}{{}"{}}


\newcommand{\e}[2]{e_{#1}^{(#2)}}

\newcommand{\f}[2]{f_{#1}^{(#2)}}


\usepackage{amsthm}



\newcommand{\ifff}{if and only if }


\renewcommand\qedsymbol{$QED$}


%\footskip = 0pt

%\setlength{\parindent}{0cm}

\usepackage{titlesec}
\titleformat{\section}[block]{\color{black}\Large\bfseries\filcenter}{}{1em}{}

\date{}
\begin{document}

\section*{Exercise 1}

\subsection*{1}
\noindent

Let $x \in M$, then we can find a neighbourhood $U$ of $x$ and a chart $x:U\rightarrow \mathbb{R}^n$,
and $\{\frac{\partial}{\partial x_i}\}$ be the induced basis on $TU$.

Then locally, we can write:
$$
X=\sum x_i {\frac{\partial}{\partial x_i}}
;\quad
Y=\sum y_j {\frac{\partial}{\partial x_j}}
$$

Then a simple calculation yields:
$$
XYf
=
\sum x_i {\frac{\partial y_j}{\partial x_i}}{\frac{\partial f}{\partial x_j}}
+
\sum
x_i
y_j
\frac
{\partial^2 f}
{\partial x_j \partial x_i}
;\quad
YXf
=
\sum y_j {\frac{\partial x_i}{\partial x_j}}{\frac{\partial f}{\partial x_i}}
+
\sum
x_i
y_j
\frac
{\partial^2 f}
{\partial x_j \partial x_i}
$$

$$
[X,Y]
=
\sum \left(
x_i {\frac{\partial y_j}{\partial x_i}}
-
y_j {\frac{\partial x_i}{\partial x_i}}
\right)
\frac{\partial f}
{\partial x_j}
$$

Clearly, $B_x(X,Y)$ is symmetric \ifff $[X,Y]_x=0$. Since 
by definition of $B$,
$$B \cap U=
\left\{
x\in U
\left\vert
\frac
{\partial f}
{\partial x_j}
(x)
=
0
\right.
\right\}
$$
%$
%\frac
%{\partial f}
%{\partial x_j}
%(x)
%=
%0
%,
%\forall x\in B
%, \forall j \in\{1,\ldots,\text{dim } M\}$.
We get that if $x\in B$ then:

\begin{equation}
\label{eq:hessian1}
(XYf)_x
=
\sum
x_i
(x)
y_j
(x)
\frac
{\partial^2 f}
{\partial x_j \partial x_i}
(x)
;\quad
(YXf)_x
=
\sum
x_i(x)
y_j(x)
\frac
{\partial^2 f}
{\partial x_j \partial x_i}
(x)
;\quad
[X,Y]_x
=
0
\end{equation}

 Hence the symmetry of $B_x(X,Y)$ is proven and from \eqref{eq:hessian1} is clear that is only depends on $x_i(x)$ and
 $y_i(x)$, that is: $B_x(X,Y)$ only depends on $X_x$ and $Y_x$.
 
 Finally, if $B$ is a submanifold, then $X\in TB$, $X$
 can be integrated to obtain a curve $x(t)$ contained in B so that $X_{x(t)}=x'(t)$.
 
Since $x(t)\in B$ we have $\frac{\partial f}{\partial x_i}(x(t))=0$
, 
we can differentiate to obtain:
$$
% \frac{d}{dt}
0
=
%\sum x_i'(t)
%\frac{\partial f}{\partial x_i}
%+
\sum
_j
x_j'(t)
\frac{\partial^2 f}{\partial x_i x_j}
\quad,\forall i\in\{1,\ldots,n\}
$$

Since $x_i'(0)=x_i(x)$, by the above calculations we have that:

$$
(XYf)_x
=
\sum_{i,j=1}^n
x_i(x)
y_j(x)
\frac
{\partial^2 f}
{\partial x_j \partial x_i}
(x)
=
\sum_{j=1}^n
\left\{
y_j(x)
\sum_{i=1}^n
x_i(x)
\frac
{\partial^2 f}
{\partial x_j \partial x_i}
\right\}
=
0
$$

This proves the case $X_x\in T_xB$.
The case $Y_x\in T_xB$ is completely analogous, since
for $x\in B, (XYf)_x=(YXf)_x$
 
 \subsection*{2}
 \noindent
 
 Let $H_x(f)(u,v)\defequal (XYf)_x$, where we choose $X,Y$ so that $u=X_x$ and $v=Y_x$.

We have to check that the choice of $X,Y$ does not alter the value of H, however this was proven in the previous section where we checked that $(XYf)_x$ only depended on $X_x$ and $Y_x$.

Finally, since:
\begin{equation}
\ker H_x(f)
=
\{
X\in TM
\vert
H_x(f)(X,Y)=0,
\forall Y
\}
=
\{
X\in TM
\vert
(YXf)_x
=0,\;\;
\forall Y
\}
=
TB
\end{equation}

we deduce that the hessian of f is non-degenerate on $N_{{B/M},x}$
. 

Finally, we can choose $U$ and a local frame 
$\mathcal{B}=\frac{\partial}{\partial x_j}$
on $U$ so that $H_x(f)$
has an associated symmetric matrix $M$
so that 

$$
H_x(f)
\left(\sum x_i\frac{\partial}{\partial x_i},
\sum y_j\frac{\partial}{\partial x_j}
\right)
=\sum_{i,j}x_iy_jM_{ij}
$$

Then if we change the frame
$\frac{\partial}{\partial x_j}$ to another frame
$\frac{\partial}{\partial y_j}$ we have a matrix $P$ 
a linear transformation on $TM$ so that
$P\frac{\partial}{\partial y_j}=\frac{\partial}{\partial x_j}$.

Then the matrix $M^{'}$ associated to the Hessian by the new matrix is given by:

$$
M^{'}=
P^TMP
$$

By Sylvester's inertia theorem,
we can find an invertible matrix $S$,
so that
$
S^TMS
$
is diagonal.

By the above discussion, it's enough to choose 
$\{S^{-1}\frac{\partial}{\partial x_i}\}$, 
as in this frame the Hessian has the form of $(0.5)$.

It also clear, that since each connected component $C_j$ of $B$
is a submanifold where $df\vert_{C_j}=0$ then $f\vert_{C_j}=const$.

\subsection*{3}

\begin{itemize}[label={}]

\item a)

By the usual Poincaré lemma, we can find a form $\gamma$ so that
$\gamma\vert_W=0$ and $d\gamma=\alpha$.

%By a theorem of the course, the inclusion $i:\Omega^k(M)^G\rightarrow\Omega^k(M)$
%is a quasi-isomomorphism. That is, we can find a equivariant form $\beta$,
%so that
%$\gamma-\beta$ is exact, in particular $d\beta=\alpha$. Furthermore, since the inclusion is linear,
%we must have that
%$\beta\vert_W=0$.
%
%So $\beta$ is the form we were looking for.

Since the action is trivial on $W$ we can find a 
$G$-invariants neighbourhood of $W$
that we shall call $U$.
Then we apply the standard trick of averaging $\gamma$ over $U$. Let $\beta$
we defined as

$$
\beta
=
\int_U g^*\gamma d\mu
$$

Where we chose $\mu$ to be a Haar measure, as it is well-know that
compact Lie groups admits such a measure.
And we could normalize in the usual way by requiring
$
\int_U d\mu
=
1
$.

Then since the action is trivial on $W$, $\beta\vert_W=0$ and
since $d$ commutes with the integral sign and with the pullback 
by the naturality condition we have that:

$$
\begin{aligned}
d\beta
&=
d\int_U g^*\gamma d\mu
=
\int_U g^*(d\gamma) d\mu\\
&=
\int_U g^*\alpha d\mu
\stackrel{(*)}{=}
\int_U \alpha d\mu
=
\alpha \int_U d\mu\\
&=\alpha
\end{aligned}
$$

Where $(*)$ follows because $\alpha$
is invariant by hypothesis.

Furthermore, $\beta$ is invariant by construction.
This proves the theorem.

\item b)

We follow the proof of the standard Darboux's lemma, with a slight twist:

Let $\omega_t=(1-t)\omega_0
+t\omega_1$. Then clearly, $\omega_t\vert_W=\omega_0\vert_W$.
Then let $X_t$ be an arbitrary differential vector field,
and let $\phi_t$ be it's flow. Then:

\begin{equation}
\begin{aligned}
\label{eq:moser}
\frac{d}{dt}\phi^*_t\omega_t
&=
\phi^*
(
L_{X_t}\omega_t
+
\omega_1
-
\omega_0
)
=0
\\
&\iff
L_{X_t}\omega_t
+
\omega_1
-
\omega_0
=0
\end{aligned}
\end{equation}

Then, because the action is trivial on $W$
we can find a neighbourhood $U$ of $W$
so that the action of $G$ is well defined. 
That is $g\cdot U
\subset U,\forall g\in G$.

By part a) we can, shrinking $U$ if necessary 
find a $G$-invariant form $\beta$
so that $d\beta=\omega_1-\omega_0$ and
$\beta_W=0$, since $\omega_1-\omega_0$
is in the hypothesis of a).

Then, using Cartan's formula, since $\omega_t$ is closed we get:
$L_{X_t}\omega_t=di_{X_t}\omega$.

So \eqref{eq:moser} turns into:
\begin{equation}
\label{eq:resb}
i_{X_t}\omega_t
+
\beta=0
\end{equation}

Because $\omega_t$ is not degenerate, because
it is the convex combination of two symplectic forms,
we can solve \eqref{eq:resb} to get a 
vector field $X_t$ that solves $\eqref{eq:moser}$,
since $\frac{d}{dt}\phi^*_t\omega_t=0
\implies
\phi^*_1\omega_1=\omega_0
$


If we could show that $\phi_t$
is $G$-invariant we would be done. We are going to show that
if we define $\hat{\phi}_t=g\cdot\phi_t(g^{-1}\cdot x)$. 
By the uniqueness guaranteed by the Picard-Lindelöf theorem we have to show that:

$$
\begin{cases}
\frac{\partial}{\partial t}\hat{\phi}_t=X_t\hat{\phi}_t\\
\hat{\phi}_0(x)=x
\end{cases}
$$

For the first part:

\begin{equation}
\begin{aligned}
\frac{\partial}{\partial t}\hat{\phi}_t
=
dg
X_t
\phi_t(g^{-1} \cdot x)
&=
dg
X_t
g^{-1}\cdot g
\phi_t(g^{-1} \cdot x)\\
&=
dg
X_t
g^{-1}
\hat{\phi_t}( x)
=
X_t
\hat{\phi_t}(x)
\end{aligned}
\end{equation}

Where last equality comes from the $G$-invariance of $X_t$
which in turn comes from the $G$-invariance of $\omega_t$.


For the second part: 
$
\hat{\phi}_0
=
g\cdot\phi_0(g^{-1}\cdot x)
=
g\cdot g^{-1} \cdot x
=x
$


And so the invariant Darboux theorem 
follows.

\end{itemize}


%\subsection*{4}

\section*{Exercise 2}

\subsection*{1}

We have that $\omega$ is invariant by hypothesis,
furthermore, since both M and $\mathbb{T}$ are compact we can
find $\langle\cdot,\cdot\rangle$ a bi-invariant scalar product
by the standard averaging procedure.

Then there is a unique matrix $A$ so that:

\begin{equation}
\label{eq:defA}
\omega(
X,
Y)
=
\langle
AX,Y
\rangle
,\quad
\forall X,Y\in TM
\end{equation}

Since both $\omega$ and $<\cdot,\cdot>$ are invariant, we have that by uniqueness,
so is $A$, that is because $g^{-1}Ag$ also verifies \eqref{eq:defA} as we show.
Let $g\in G$, then:

$$
\langle
AX,Y
\rangle
\stackrel{\eqref{eq:defA}}{=}
\omega(X,Y)
\stackrel{
*
}{=}
\omega(dg(X),dg(JY))
=
\langle
Adg(X),dg(Y)
\rangle
\stackrel{*}{=}
\langle
dg^{-1}Adg(X),Y
\rangle
$$

Where $*$ follows by the invariance of $\omega$ and $\langle \cdot,\cdot\rangle$

By the skew-symmetry of $\omega$ we have that $A=-A^T$, therefore $-A^2=AA^T$
is symmetric and we can take it's square root. (By diagonalising $-A^2=
Pdiag(\lambda_1,\ldots,\lambda_n)P^{-1}$, 
then $\sqrt{-A^2}=
Pdiag(\sqrt\lambda_1,\ldots,\sqrt\lambda_n)P^{-1}$)

Then 
$
J=
\left(
\sqrt{- A^2}
\right)^{-1}A
$
is invariant, because $A$ is
and is the composition of $G$-invariant homomorphism.

Clearly, $J^2=- A^{-2}
A^2=-Id$
and $J$ is compatible with $\omega$ because:

$$
\begin{aligned}
\omega(X,JX)
&=
\langle
AX,JX
\rangle\\
&=
\left\langle
AX,
\left(
\sqrt{- A^2}
\right)^{-1}A
X
\right\rangle
=
\left\langle
X,
\left(
\sqrt{- A^2}
\right)^{-1}
X
\right\rangle
>0
\end{aligned}
$$

Where the last inequality follows because $\sqrt{- A^2}^{-1}$
is definite positive.

$J$ is therefore the almost complex structure we were looking for.

\subsection*{2}

Since $g^{TM}$ is $G$-invariant,
$G$ acts by isometry, and takes geodesics to geodesics.
Let $\gamma(t,x_0,v_0)$
be the unique geodesic starting in $x_0$ with derivative in 0 $v_0$.
Then $exp_{x_0}(v_0)=\gamma(1,x_0,v_0)$

Therefore, if $x\in M^\mathbb{T}$, let's consider
 $\alpha(t)=g\cdot \gamma(t,x_0,v_0)$. By the above discussion, 
 $\alpha(t)$ is also a geodesic. 
 
 Since $\alpha(0))=g\cdot x_0=x_0$ and 
 $\alpha'(0)=dg_{x_0}(\gamma'(0,x_0,v_0))
 =dg_{x_0}(v_0)
 $
 
By uniqueness: $\alpha(t)=\gamma(t,x_0,dg_{x_0}(v_0))$.
In particular, for t=1 we get:

\begin{equation}
\label{eq:iso}
exp_{x_0}(dg_{x_0}(v_0))
=
g\cdot 
exp_{x_0}(v_0)
\end{equation}

\subsection*{3}

Let $x_0\in M^\mathbb{T}$,
then we can restrict the $U$
in the previous part to a $\mathbb{T}$-invariant neighbourhood.
In this way the $\mathbb{T}$-action on $U$ is well-defined.
Furthermore, since 
in the previous part we used an arbitrary 
$g\in \mathbb{T}$ we have that the exponential map is equivariant.

We notice that if 
$exp_{x_0}(v)\in M^\mathbb{T}$
then:
\begin{equation}
\label{eq:identify}
exp_{x_0}(v)
=
g\cdot exp_{x_0}(v)
=
exp_{x_0}(dg_{x_0}v),
\quad
\forall g\in \mathbb{T}
\end{equation}

This mens we can identify fixed points of the action in $U$
 with fixed point of the differential of the action on $T_{x_0}M$, since
 for $v\in T_{x_0}M, g\in\mathbb{T}$, $v=dg_{x_0}v$ because
 of \eqref{eq:identify} and $exp$ being an isomorphism.
 
That is, the exponential map send the eigenspace of $T_{x_0}M$ of eigenvalue 1 
to $M^\mathbb{T}\cap U$. Because the exponential map is a diffeomorphism it sends
submanifolds to submanifolds, in particular, it sends linear subspaces to submanifolds.

In summary:
Every point in $M^\mathbb{T}$
admits a neighbourhood that is parametrised by the exponential
map.

%Then we can restrict $U$ to a coordinate neighbourhood by a map
%$\phi$ so we have the following diagram:

%$$
%\begin{tikzcd}
%x^{-1}
%(
%\cap_{g\in\mathbb{T}} 
%\ker (Id-g)
%) \arrow{r}{d\phi_{\phi^{-1}(x_0)}}
%& T_{x_0}M \arrow{r}{exp_{x_0}}
%& C \arrow[r]
%& \cdots
%\end{tikzcd}
%$$

And so $M^\mathbb{T}$ is a submanifold.

\subsection*{4}


Because $x_0$ is a fixed point,
the action of $\mathbb{T}$ on $M$ lift to $T_{x_0}M$
via the differential, in particular, given $g,h\in\mathbb{T}$:

$$
d(gh)_{x_0}
=
dg_{x_0}dh_{x_0}
$$

By the chain rule. (We reiterate $x_0$ is a fixed point). 
Furthermore, since the action acts by symplectomorphism (by defintion),
we have that because $x_0$ is a fixed point if we choose
$g\in \mathbb{T}$:
$$
\omega_{x_0}(X,Y)
=
\omega_{g\cdot x_0}
(dg_{x_0}X,dg_{x_0}Y)
=
\omega_{x_0}
(dg_{x_0}X,dg_{x_0}Y)
$$

This proves that the action on $T_{x_0}M$
is symplectic. It only remains to prove that the action commutes with $J$,
however this is the definition for invariance of $J$ which we already prove.

\subsection*{5}

At this point we suspect that $\varphi$ might be given
by the exponential map with the natural identification of $B(x_0,\delta)$
with $\mathbb{R}^{2n}$, that is, under indentification: $\varphi=exp_{x_0}^{-1}$.
This is obviously a diffeomorphism and it is $\mathbb{T}$-equivariant because
we have defined precisely the action as the induced by this diffeomorphism.

Furthermore, by the section on compact Lie groups we know that there 
is a natural identification of $\mathbb{T}^k$ with $(\mathbb{S}^1)^k$
given by:
$$
exp(\tau)
=
(
e^{i\tau_1}
,\ldots,
e^{i\tau_k}
)
$$

And so the action induced on $V$ must be of the above form, since it's linear
and acts by elements of $U(n)$.

\subsection*{6}
We define $A_g\defequal \ker(\mathbbm{1}-dg_{x_0}g)$
to ease notation.

Let $v\in A_g$
we have that because of the previous parts
of the exercise:

\begin{equation}
\label{eq:sub}
v=
dg_{x_0}(J_{x_0}v)
=
J_{x_0}dg_{x_0}(v)
=
J_{x_0}v
\end{equation}

And so 
$J_{x_0}A_g=
A_g
$, because by \eqref{eq:sub} we have proved the $\subset$
inclusion and the other inclusion comes from the fact that
$J$ is an isomorphism since $-J$ is it's inverse.

From which we conclude that:

\begin{equation*}
T_{x_0}(M^\mathbb{T})
=
\left(
T_{x_0}M
\right)^\mathbb{T}
=
\bigcap_{g_\in\mathbb{T}}
A_g
\end{equation*}

If we showed that each of the $A_g$ was symplectic, then
theirs intersection would be as well and the result would follow.
That is the proof follows from this two claims:

{\bf Claim}: $A_g$ is symplectic:
{\bf Claim}: The intersection of symplectic spaces is symplectic.

\begin{proof}(First claim)

By way of contradiction.
Let $g$ be an arbitrary element in $\mathbb{T}$
and $v\in A_g$ so that $i_v\omega_{x_0}=0$.

Because of $J_{x_0}A_g=
A_g
$
we know we can find $w$
so that $J_{x_0}w=v$. Then:
%Let $u\in A_g$ we an arbitrary element, then:

$$
0=\omega_{x_0}(J_{x_0}w,w)
=
-g^{TM}_{x_0}(w,w)>0
$$

Which is a contradiction.
\end{proof}


\begin{proof}(Second claim)
$A_g$ is symplectic if and only if
$A_g\cap A_g^{\bot\omega}=\{0\}$.

From this we get:

\begin{equation*}
\begin{aligned}
\left(
\bigcap_{
g\in\mathbb{T}
}
A_g
\right)
\cap
\left(
\bigcap_{
g\in\mathbb{T}
}
A_g
\right)^{\bot\omega}
&=
\left(
\bigcap_{
g\in\mathbb{T}
}
A_g
\right)
\cap
\left(
\bigcup_{
g\in\mathbb{T}
}
A_g^{\bot\omega}
\right)
\\
&=
\bigcap_{h\in\mathbb{T}}
\left(
\left(
\bigcap_{
g\in\mathbb{T}
}
A_g
\right)
\bigcap
\left(
\bigcup_{
h\in\mathbb{T}
}
A_g^{\bot\omega}
\right)
\right)
\\
&=\{0\}
\end{aligned}
\end{equation*}

And we are done.

\end{proof}

In summary we have proven that
$
T_{x_0}(M^\mathbb{T})
$
is symplectic.

Since we have already shown that the exponential map is a symplectomorphism
and that it maps
$
T_{x_0}(M^\mathbb{T})
$ to 
$
\left(
T_{x_0}M
\right)^\mathbb{T}
$ we conclude
$
M^\mathbb{T}
$ 
is a symplectic submanifold.
\subsection*{7}

We have to prove two things:

\begin{enumerate}

\item $d(\mu,\tau)=i_\tau^{\mathbb{C}^n}\omega_{st}$.

\begin{proof}
We begin by seeing who is $\tau^{\mathbb{C}^n}$. 
Let $z=x+iy\in\mathbb{C}$ then:

\begin{align*}
\tau^{\mathbb{C}^n}\vert_z
=
\frac{d}{ds}
exp(s\tau)\cdot z
&=
(
-i\langle w_1,\tau\rangle z_1,
\ldots,
-i\langle w_n,\tau\rangle z_n
)
\\
&=
(
\langle w_1,\tau\rangle y_1,
\langle w_1,\tau\rangle x_1,
\ldots,
\langle w_1,\tau\rangle y_n,
\langle w_1,\tau\rangle x_n,
)\\
&=
\sum_{j=1}^n
\langle
w_j,\tau
\rangle
\left(
y_j\frac{\partial}{\partial x_j}
-x_j\frac{\partial}{\partial y_j}
\right)
\end{align*}

Again, by computation we get:

$$
i_{y_j\frac{\partial}{\partial x_j}
-x_j\frac{\partial}{\partial y_j}}
(dx_j\wedge dy_j)
=
x_jdx_j
+
y_jdy_j
$$

And gluing all together:
\begin{equation}
\label{eq:inclusion}
i_{\tau^{\mathbb{C}^n}}\omega_{st}
=
\sum_{j=1}^n
\langle
w_j,\tau
\rangle
(
x_jdx_j
+
y_jdy_j
)
\end{equation}

Now be go the LHS, let's see who is 
$d(\mu,\tau)$. Let $z\in\mathbb{C}$:

\begin{align}
\label{eq:momentum}
d(\mu_0(z),\tau)
=
d\left(
\frac{1}{2}
\sum_{j=1}^n
(x_j^2+y_j^2)
(w_j,\tau)
\right)
=
\sum_{j=1}^n
(x_jdx_j+y_jdy_j)
(w_j,\tau)
%=
%i_\tau^{\mathbb{C}^n}\omega_{st}
\end{align}

Joining \eqref{eq:inclusion} with \eqref{eq:momentum} we get the result.

\end{proof}

\item $\omega_{st}$ is equivariant, i.e
$\mu_0(g\cdot z)=Ad_{g}^*\mu_0(z)$.

\begin{proof}
Firstly, we notice that since $\mathbb{T}$ is abelian, so is it action, in the
sense that $Ad_{g}$ acts by identity. So the RHS is just $\mu(z)$.

For the LHS, we notice that since $\mathbb{T}$ is compact, it is generated as a group
by $\{exp(\tau),\tau\in\mathfrak{g}\}$, so if we show that 
$\mu_0(\exp(\tau)\cdot z)=\mu_0(z)$ this would that the LHS is equal to $\mu_0(z)$
for any $z\in\mathbb{T}$.

We show the final claim:

$$
\mu_0(\exp(\tau)\cdot z)
=
\frac{1}{2}
\sum_{j=1}^n
\vert
e^{-i\langle
w_j,\tau
\rangle
}
z_j
\vert^2
w_j
=
\frac{1}{2}
\sum_{j=1}^n
\vert
z_j
\vert^2
w_j
=
\mu_0(z)
$$


\end{proof}

\end{enumerate}

\subsection*{8}

By definition of moment map we have:
$$
d(\mu,\tau)
=
i_{\tau^M}\omega
$$

And unraveling the identifications in $\mu_0$
we have that we actually have $\varphi^*\mu_0$,
where $\varphi$ is the diffeomorphism above.

$$
d(\varphi^*\mu_0,\tau)
=
\varphi^*i_{\tau^{\mathbb{C}^n}}\omega_{st}
=
i_{\varphi^*\tau^{\mathbb{C}^n}}\varphi^*\omega_{st}
=
i_{\varphi^*\tau^{\mathbb{C}^n}}\omega
$$

By the non-degeneracy of $\omega$ we have that
$$d(\mu-\mu_0)=0
\iff i_{\tau^M}\omega-i_{\varphi^*\tau^{\mathbb{C}^n}}\omega
\iff
\tau^M
=
\varphi^*\tau^{\mathbb{C}^n}
$$

We can show that the last equality holds by direct computation:
\begin{align*}
\varphi^*\tau^{\mathbb{C}^n}\vert_x
&=
d\varphi^{-1}\left(
\left.\frac{d}{ds}
\varphi
exp(s\tau)
\cdot 
x
\right\vert_{s=0}
\right)
\\
&=
d\varphi^{-1}\left(
d\varphi\left(
\left.\frac{d}{ds}
exp(s\tau)
\cdot 
x
\right\vert_{s=0}
\right)
\right)
\\
&=
\left.\frac{d}{ds}
exp(s\tau)
\cdot 
x
\right\vert_{s=0}
=
\left.\tau^M\right\vert_x
\end{align*}

Thus $d(\mu-\mu_0)=0$, 
which implies that $(\mu,\tau)-(\mu_0,\tau)$
is locally constant for any $\tau$. Since are working on a connected neighbourhood, 
we deduce that 
$
(\mu,\tau)=(\mu(x_0),\tau)+(\mu_0,\tau)
$.

Finally, since $\tau$ is arbitrary we conclude 
$
\mu=\mu(x_0)+\mu_0
$ which is what we wanted. (Notice that with the identification of $\mu$,
we have that $\mu(0)=\mu(x_0)$).

\subsection*{9}

By definition
$Crit(\mu_X)=
\{
x\in M:
d\mu_x
=0
\}
$
Then by the definition of momentum map we get: 
\begin{equation}
\label{eq:tangent}
\begin{aligned}
d(\mu_X)_x&=0\\
&\iff
d(\mu_X)_x(Y)=0,\quad \forall Y 
\in C^\infty(M,TM)\\
&\iff
\omega(X^M_x,Y)=0,
\quad \forall Y
\in C^\infty(M,TM)\\
&\iff
X^M_x=0
\end{aligned}
\end{equation}

The last equality follows from the 
non-degeneracy of $\omega$.

We notice that $X_x^M=0$ if and only if $x$ is a fixed point of the action, that is:

$$
Crit(\mu_x)
=
M^\mathbb{T}
$$

And so 
$Crit(\mu_x)$
is a symplectic submanifold because we have already proven it in $(6)$,

\subsection*{10}

%It follows from the
%previous section that,
%since $M$ is compact, $Crit(\mu_X)$ has finitely many connected components, 
%so they are open, and therefore submanifolds themselves.

It follows from the previous section that 
$
Crit(\mu_x)
=
M^\mathbb{T}
$ 
and 
by $(6)$ $Crit(\mu_x)$ is symplectic submanifold.

The Hessian can be calculated as follows:
$
H_x(\mu_X)(Y,Z)
=
(ZY\mu_X)_x
$

Then:
$$
\begin{array}{l}
Y\mu_X =
d\mu_X(Y)=\omega(X^M,Y)\\
ZY\mu_X =
d(\omega(X^M,Y)(Z))
=
(i_{[Y,X^M]}\omega)(Z)
=
\omega
(
[Y,X^M]
,
Z
)
=
\omega
(
L_Y(X^M)
,
Z
)
\end{array}
$$

It follows that:
$
H_x(\mu_X)(Y,Z)
=
\omega_x
(
[Y,X^M]
,
Z)
$.

Since $[Y,X^M]=L_{Y}X^M=
\left.\frac{d}{dt}\right\vert_{t=0}exp(-tY)_*X^M$.
Which means by \eqref{eq:tangent}
that:
$$
TCrit(\mu_X)
=
\{
Y\in TM:
L_{Y}X^M=0
\}
=
\bigcap_{Y\in TM}\ker H_x(\mu_X)(Y)
$$

This proves the Hessian is non-degenerate on the set of critical points.
Bringing this together with the fact that $Crit(\mu_X)$
is a submanifold  we conclude that $\mu_X$ is a Morse-Bott function.


\subsection*{11}


By (0.10) in the exam, we have that he hessian of $\mu$ as identified in $\mathbb{C}^n$
and then identified again in $\mathbb{R}^{2n}$ is
 $\frac{1}{2}\sum_{j=1}^n(x_j^2+y_j^2)\omega_j(X)$. 
 
 Where by the $\mathbb{C}^n$ to $\mathbb{R}^{2n}$
identification we mean identifying $z=x+iy$ with $(x,y)$.

This means the matrix of the Hessian is as follows:
$$
\begin{pmatrix}
\lambda_1 & 0 & \cdots & 0 &0\\
0 & \lambda_1 & \cdots & 0&0\\
\vdots & \vdots & \ddots & \vdots&\vdots\\
0 & 0 & \cdots& \lambda_n&0\\
0 & 0 & \cdots & 0&\lambda_n\\
\end{pmatrix}
$$

Where $\lambda_i\defequal(w_i,X)$.
That is every eigenvalue appears twice due to the identification.
So there must a even number of them that are negative (maybe $0$, but even nonetheless).
Which proves the result.

\section*{Exercise 3}

\subsection*{1}

By non-degeneracy of $\omega$
it is enough to show that for a given $X\in TM$
we have:

\begin{equation}
\label{eq:31}
\omega
\left(\mu^M,X
\right)
=
\omega
\left(-\frac{1}{2}
J(d\mathcal{H})^*))
,
X
\right)
\end{equation}

On the LHS we have:
\begin{equation}
\begin{aligned}
\omega
\left(\mu^M,X
\right)
=
i_{\mu^M}\omega(X)
=
d
\left(
\langle
\mu,
\mu(x)
\rangle_\mathfrak{g}
\right)(X)
\end{aligned}
\end{equation}


On the RHS we have:

\begin{equation}
\begin{aligned}
\omega
\left(-\frac{1}{2}
J(d\mathcal{H})^*))
,
X
\right)
&=
\frac{1}{2}
g^{TM}
\left(
X,
(d\mathcal{H})^*
\right)
\\
&=
\frac{1}{2}
(d\mathcal{H})(X)
\\
&=
d(\langle
\mu,
\mu(x)
\rangle_\mathfrak{g})(X)
\\
%&=
%-
%\langle
%X\mu(x),
%\mu(x)
%\rangle_\mathfrak{g}
\end{aligned}
\end{equation}

%Where the last equality follows from Leibniz rule, which 
%we can apply because $\mathfrak{g}$ is a f

From \eqref{eq:31} we get that $\mu^M_x=0\iff
d\mathcal{H}_x=0$
and so we get:
$$
Crit(\mathcal{H})
=
\{
x\in M:
\mu_x^M=0
\}
$$

As we wanted.

\subsection*{2}

First we clarify the way $\mu_T$ is defined, which is by
the following diagram:

$$
\mu_T:
\begin{tikzcd}
M \arrow{r}{\mu}
& \mathfrak{g^*} \arrow{r}{h}
& \mathfrak{g} \arrow{r}{i^*}
& \mathfrak{t^*} \arrow{r}{h^{-1}\vert_{\mathfrak{t^*}}}
& \mathfrak{t}
\end{tikzcd}
$$

Where $i$ is the natural inclusion:
$
\mathfrak{t}
\stackrel{i}{\to}
\mathfrak{g}
$
and $h$ is the natural pairing
$
\mathfrak{g}
\stackrel{h}{\to}
\mathfrak{g^*}
$
given by $h(u)(v)=\langle u,v\rangle_\mathfrak{g}$.

Equivariance of $\mu_T$ is trivial since 
if $\mu(g\cdot x)=Ad_g^*\mu(x)$ is true for any $g\in G$
then it certainly true for any $g\in T\subset G$.

By the same logic, if $X\in T$ then $X^M$
coincides if seen as an element of $T$
or as an element of $G$ because it is defined in the same exact manner,
and because $i^*X=X$ we have that
$(\mu,X)=(\mu_T,X)$ so we have that:

$$
i_{X^M}\omega=d(\mu_T,X)
$$

Which proves $\mu_T$ is a moment map.


Let $\mathcal{B}=\{
t_1,\ldots,t_n,
x_1,\ldots,x_m
\}$
be an orthonormal basis of
$\mathfrak{g}$ and 
$\mathcal{B}^*=\{
t_1^*,\ldots,t_n^*,
x_1^*,\ldots,x_m^*
\}$ it's dual basis of
$\mathfrak{g}^*$.
Notice that because of the orthonormality condition
the dual basis coincides in the usual sense and in the scalar product sense.

Then given $x\in M$, we have:
$$
\mu(x)
=
\sum_{i=1}^n
u_i
t_i
+
\sum_{j=1}^m
v_j
x_j
$$

Then because $i^*x_j=x_j\vert_\mathfrak{t}=0$ we have:
$$
\mu_T(x)
=
\sum_{i=1}^n
u_i
t_i
=
P^\mathfrak{t}
\left(
\sum_{i=1}^n
u_i
t_i
+
\sum_{j=1}^m
v_i
x_i
\right)
=
P^\mathfrak{t}
\mu(x)
$$

\subsection*{3}

This is a direct consequence of Theorem 1.
Since Theorem 1 guarantees that $\mu_T(M^T)$
are the vertices of a convex polytope, which by definition of polytope
means it must be finite.

\subsection*{4}
This is a direct consequence
of a classic theorem in Functional Analysis, 
however here we give a slightly simplified proof for the 
case of finite dimension and the case where the point exterior to the set is the origin.



Firstofall, if $0\in A$
we are done. If not, let:

$$
\delta=dist(0,A)
= \inf_{x\in A}\vert x\vert
>0
$$

Where the strict inequality comes from the fact that $A$ is closed.

We have to show 2 thing:

\begin{enumerate}

\item There exists $x\in A$ so that $\vert x\vert =\delta$.

\item There is only one such $x\in A$.

\end{enumerate}

\begin{proof}(Existence)

Because $\delta$ is defined as an infimum,
we can find a sequence of points $x_n$ so that
$\vert x_n\vert\to \delta$.
By picking a subsequence, we may assume $x_n$
that 
$$
x\leq \vert x_n\vert 
\leq
x+\frac{1}{n+1}
$$

Let $n,m\in\mathbb{N}$,
then by hypothesis 
$d\leq\frac{1}{2}
(y_n+y_m)\in A$.

By direct computation we have:

\begin{equation}
\begin{aligned}
\delta^2\leq
\left\vert
\frac{1}{2}y_n
+
\frac{1}{2} y_m
\right\vert^2
&\stackrel{(1)}{=}
\frac{1}{2}\left(
\left\vert
y_n
\right\vert^2
+
\left\vert
y_m
\right\vert^2
\right)
-
\frac{1}{4}
\left\vert
y_n
+
y_m
\right\vert^2
\\
&\leq
\delta^2
+
\frac{1}{2}
\left(
\left(\frac{1}{n+1}
\right)^2
+
2d\left(
\frac{1}{n+1}
+
\frac{1}{m+1}
\right)
+
\left(
\frac{1}
{m+1}
\right)^2
\right)
-\frac{1}{4}
\vert
x_n-x_m
\vert^2
\end{aligned}
\end{equation}
From which we get:

\begin{center}
\begin{tabular}{l}
$
\vert
x_n-x_m
\vert^2
\leq
2
\left(
\left(\frac{1}{n+1}
\right)^2
+
2d\left(
\frac{1}{n+1}
+
\frac{1}{m+1}
\right)
+
\left(
\frac{1}
{m+1}
\right)^2
\right)
$
\\
$
\vert
x_n-x_m
\vert^2
\stackrel{n,m\to\infty}\longrightarrow
0
$
\end{tabular}
\end{center}

Which means $\{x_n\}$ is a Cauchy sequence and therefore convergent, 
because $A$ is closed we get that the limit is in $A$ and obviously the limit has 
norm $\delta$ by the continuity of the norm.


\end{proof}

\begin{proof}(Uniqueness)
Suppose $x,x'\in A$, $x\neq x'$, so that 
$
\vert x\vert
=
\vert x'\vert
=
\delta
$

Let 
$y = \frac{1}{2} x +(1-\frac{1}{2} x')$.
By hypothesis $y\in A$ and so $\vert y\vert \geq \delta$

And so by use of the parallelogram identity:

$$
\delta^2
\leq
\frac{1}{2}
\leq
\left\vert \frac{1}{2} x+
(1-\frac{1}{2})x'
\right\vert^2
=
2
\left(
\left\vert \frac{1}{2} x
\right\vert^2
+
\left\vert
\frac{1}{2}x'
\right\vert^2
\right)
-
\frac{1}{2}
\left\vert  x-x'
\right\vert^2
=
\delta^2
-
\frac{1}{2}
\left\vert  x-x'
\right\vert^2
<
\delta^2
$$

Where the last inequality comes because $\left\vert  x-x'
\right\vert^2
>0$.

\end{proof}

\subsection*{5}

By definition each element  $b$ of $\mathcal{B}$
is associated to a subset of $\mathcal{P}(\mathcal{A})$
which is finite because $\mathcal{A}$ is finite. In particular,
$\#\mathcal{P}(\mathcal{A})=
2^{\mathcal{\#A}}$ so:
$$
\#\mathcal{P}(\mathcal{A})<\infty
\iff
\#\mathcal{A}<\infty
$$

Since this association of $\mathcal{B}$
with subsets of $\mathcal{A}$ gives and injection we have:
$$
\#\mathcal{B}
\leq
\mathcal{P}(\mathcal{A})
<\infty
$$

Which is what we wanted.

\subsection*{6}

We already know by the previous part of the exercise that
$Crit(\mu_\beta)
=
\{
x\in M\vert
\beta_x^M=0
\}
$.

The set
$Z_\beta
=
\{
x\in M\vert
\beta_x^M=0
,\;
\mu_\beta(x)
\vert
\beta\vert^2
\}
$ is clearly closed in $Crit(\mu_\beta)$
by continuity of $\vert\cdot\vert^2$.
Furthermore, since $\mu_\beta$ must be locally constant along
the connected components of 
$Crit(\mu_\beta)$ we conclude
$Z_\beta$ must be relatively open in $Crit(\mu_\beta)$.

This means $Z_\beta$ is clopen in $Crit(\mu_\beta)$ 
and so is the union of open connected components of $Crit(\mu_\beta)$ 
(we know they are open because there are only a finite amount of them due 
to $M$ being compact).

In particular, this means $Z_\beta$ is the disjoint union of symplectic manifolds.
Since being symplectic is a local condition (it is a condition on the differential, which is local
and on the tangent space on a point, which obviously local) we conclude
that $Z_\beta$ must be symplectic.

\subsection*{7}

Since for $\mu(x)\in\mathfrak{t}^*$
$\mu(x)=\mu_T(x)$ by $(2)$

From this we have:
$$
x\in Crit(\mathcal{H})
\iff
x\in Crit(\vert\mu\vert^2)
\iff
x\in
Crit(\vert\mu_T\vert^2)
$$

We now prove the following: {\bf Claim}:
Given $x\in M$ with $\mu_T(x)=\beta\in\mathfrak{t}$.
Then we have that $x\in Crit(\mathcal{H})$ if and only if
$x\in Z_\beta$. And when this happens $\beta$ is the element of minimal norm in $\mu_t(Z_\beta)$
which is obviously a convex set.

\begin{proof}
We have the following:

$$
x\in Crit(\mathcal{H})
\iff
\left.\mu^M\right\vert_x=0
\iff
\beta^M_X=0
$$

Where the last double implication comes from exercise 2 part 9 (and the definition).

And so the first implication ($\Rightarrow$) follows by the definition of $Z_\beta$.

For the $\Leftarrow$ implication we know by Theorem 1
that $\mu_T(Z_\beta)
=
\text{convex hull}(\mu_T(Z_\beta^T))$

By the previous part there exits a element of minimal norm 
in $\mu_T(Z_\beta)$.
We only have to show that this point is $\beta$.
Let's argue by way of contradiction, let $\gamma$
be the point of minimal norm, in particular,
$\vert
\gamma\vert<\vert \beta\vert$
and we can find $x\in Z_\beta$
so that $\gamma=\mu(x)=\mu_T(x)$, where 
the last equality holds because $\mu_t(Z_\beta)\subset\mathfrak{t}$.

We have that by definition of $Z_\beta$ (and by the identification of $\mathfrak{t}$
with $\mathfrak{t^*}$) that
$
\langle
\mu(y),
\beta\rangle
=
\langle
\beta,
\beta
\rangle
$

from which trivially we have:

\begin{align*}
&\langle
\mu(y)-\beta,
\beta\rangle
=
0
\\
&
\vert\gamma\vert^2
=
\vert\mu_T(y)\vert^2
=
\langle
\mu(y),
\mu(y)
\rangle
\stackrel{\pm\beta}{=}
\langle
\mu(y)-\beta,
\mu(y)-\beta
\rangle
+
\langle
\beta,
\beta
\rangle
=
\vert
\mu(y)-\beta
\vert^2
+
\vert\beta\vert^2
<
\vert\beta\vert^2
\end{align*}

Which is a contradiction and so we have proven the claim above.
\end{proof}

It follows immediately from the equivariance of $\mu$
that $g\cdot Crit(\mathcal{H})=Crit(\mathcal{H})$.

Putting all together we have that:

$$
g\cdot x\in Crit(\mathcal{H})
\iff
g\cdot z\in Z_\beta
\iff
x\in C_\beta
$$

Which shows: $Crit(\mathcal{H})
=
\cup_{\beta\in\mathcal{B}}C_\beta
$

It only remains to show that $C_\beta$
are disjoint. Let $\beta\in\mathcal{B}$, then:
$$
\mu(C_\beta)
=
\{
\mu(g\cdot x)\vert
x\in Z_\beta\cap\mu^{-1}(\beta),g\in G
\}
\{
Ad_g(\mu(x))
\vert
x\in Z_\beta,
\mu(x)=\beta,g\in G
\}
=
Ad_G(\beta)
=\mathcal{O}_\beta 
$$

And by a theorem of the course we know that each coadjoint orbit intersects each positive Weyl chamber number in precisely one point, this directly implies that
$C_\beta$
are disjoint since otherwise we could use the coadjoint action to bring a point
in the intersection
to a point in 
a 
positive Weyl chamber, which will mean two different coadjoint orbits intersect, 
which is imposible.

\subsection*{8}

We just showed: $
\mu(C_\beta)
=
Ad_G(\beta)
$

We can actually say more:

\begin{align*}
C_\beta
&=
\{
g\cdot x\vert g\in G,
x\in Z_\beta,
\mu(x)=\beta
\}
=
\{
g\cdot x\vert g\in G,
x\in Crit(\mathcal{H}),
\mu(x)=\beta
\}
\\
&=
\{
g\cdot x\vert g\in G,
g\cdot x\in Crit(\mathcal{H}),
\mu(x)=\beta
\}
=
\{
x\in Crit(\mathcal{H})
\vert
\exists g\in G,\;\;
\mu(x)=Ad_g(\beta)
\}
\end{align*}

As we have already seen $x\in C_\beta\implies x\in Z_\beta$
and $\mathcal{H}(x)=\vert\mu(x)\vert^2=\vert\beta\vert^2$
for any $x$ in $C_\beta$,
where again we have omitted the dual identification.

So we have proven: 
$
\mathcal{H}(C_\beta)=
\vert\beta\vert^2
$


\section*{Exercise 4}

\subsection*{1}

Let $\alpha,\beta\in\Omega^{0,k}(M)$
and $\langle
\cdot,\cdot
\rangle$
be the scalar product we have seen in class for $(0,k)$
forms.

We have to show
that: 
$
\langle
D_T \alpha,\beta
\rangle
=
\langle
\alpha,D_T\beta
\rangle
$
We compute:

\begin{align*}
\langle
D_T \alpha,\beta
\rangle
=
\langle
D+iTc(\mu^M)\alpha,\beta
\rangle
=
\langle
D\alpha
,
\beta\rangle
+
\langle
i
Tc(\mu^M)\alpha,\beta
\rangle
\end{align*}

By the lectures we know that $D$
is self-adjoint so 
$\langle
D \alpha,\beta
\rangle
=
\langle
\alpha,D\beta
\rangle
$
.
It is therefore enough to prove that 
$
i
Tc(\mu^M)
$
is self-adjoint.
Because $c(\mu^M)^*=-c(\mu^M)$,
and because $\langle \cdot,\cdot\rangle$
is hermitian it follows that:

\begin{align*}
\langle
i
Tc(\mu^M)\alpha,\beta
\rangle
=
\langle
\alpha,
-i
T(-c(\mu^M))
\beta
\rangle
=
\langle
\alpha,
i
Tc(\mu^M)
\beta
\rangle
\end{align*}

\subsection*{2}

\noindent

Obviously $\ker D_T\subset \ker D_T^2$. Let $x\in\ker D_T^2$,
then, because $D_T^2$ is self-adjoint:
$$
0=\langle
D_T^2x,x
\rangle
=
\langle
D_Tx,
D_Tx
\rangle
$$

So we conclude $D_Tx=0$ and so $\ker D_T^2\subset \ker D_T$
and $\ker D_T= \ker D_T^2$.


The last part is clear since
$\ker D_{+,T}=
\ker
D_T\vert_
{
\Omega^{0,\text{even}}
(M,L)
}
=\ker(D_T)\cap
\Omega^{0,\text{even}}
(M,L)
\subset
\Omega^{0,\text{even}}
$
and
in a completely analogous manner
$\ker D_{-,T}=
\ker
D_T\vert_
{
\Omega^{0,\text{odd}}
(M,L)
}
=\ker(D_T)\cap
\Omega^{0,\text{odd}}
(M,L)
\subset
\Omega^{0,\text{odd}}
$

\subsection*{3}

The first equality is trivial from the previous part since $\ker D_T^2=\ker D_T$.
The last equality would follow (as we will show) if
$D_T$ were $G$-equivariant.
On the one hand $D$ is $G$-equivariant as has been shown in the lectures.
$\mu^M$ is also $G$-equivariant as we have shown in the previous exercise.
So $D_T$ must be $G$-equivariant as well, and a fortiori
$D_{\pm,T}$ must be invariant by $G$. And so:

\begin{align*}
\ker D_T\vert_
{
\Omega^{0,\text{even}}(M,L)^G
}
&=
\ker D_T\cap
\Omega^{0,\text{even}}(M,L)^G
=
\ker D_T^2\cap
\Omega^{0,\text{even}}(M,L)^G\\
&\stackrel{(*)}{=}
\left(
\ker D_{+,T}\cap
\oplus
\ker D_{-,T}
\right)
\cap
\Omega^{0,\text{even}}(M,L)^G
\\
&=
\ker D_{+,T}
\cap
\Omega^{0,\text{even}}(M,L)^G
=
\ker D_{+,T}^G
\end{align*}

Where $(*)$ follows because $D_T$
decomposes as 
$D_T
=D_{+,T}
+
D_{-,T}$
And the case for $D_{-,T}$ is
completely analogous (except replacing 
$\Omega^{0,\text{even}}(M,L)^G$
with 
$\Omega^{0,\text{odd}}(M,L)^G$). 


\subsection*{4}

We have that:

\begin{align*}
D_T^2
=
\left(
D+iTc(\mu^m)
\right)
\left(
D+iTc(\mu^m)
\right)
=
D^2
+T^2
\vert
\mu^M\vert^2
+
iT
(
Dc(\mu^M)
+c(\mu^M)D
)
\end{align*}

So we have to show that we can find some $A_M$
0-order differential operator so that:

\begin{equation}
\label{eq:44}
iT
(
Dc(\mu^M)
+c(\mu^M)D
)
=
TA_M
-2iT
\sum^{\text{dim }G}_{i=1}
\mu_iL_{V_i}
\end{equation}

Let's see who is
$
Dc(\mu^M)
+c(\mu^M)D
$

\begin{align*}
Dc(\mu^M)
+c(\mu^M)D
&=
\sum^{2n}_{j=1}
c(\mu^M)
c(e_j)
\nabla^{Cl\otimes L}_{e_j}
+
c(e_j)
\nabla^{Cl\otimes L}_{e_j}
c(\mu^M)
\\
&\stackrel{(1)}{=}
\sum^{2n}_{j=1}
c(\mu^M)
c(e_j)
\nabla^{Cl\otimes L}_{e_j}
+
c(e_j)
c(\mu^M)
\nabla^{Cl\otimes L}_{e_j}
+
c(e_j)
c
(
\nabla^{TM}_{e_j}\mu^M
)\\
&=
\sum^{2n}_{j=1}
\left(
c(\mu^M)
c(e_j)
+
c(e_j)\right)
\nabla^{Cl\otimes L}_{e_j}
+
c(e_j)
c
(
\nabla^{TM}_{e_j}\mu^M
)
\\
&=
-2g^{TM}(\mu^M,e_j)
\nabla^{Cl\otimes L}_{e_j}
+
c(e_j)
c
(
\nabla^{TM}_{e_j}\mu^M
)\\
&=
-2\nabla^{Cl\otimes L}_{\mu^M}
+
c(e_j)
c(\nabla_{e_j}^{TM}
\mu^M)
\end{align*}

Where $(1)$ follows from 
$[\nabla^{Cl\otimes L}_V,c(W)]
=
c(\nabla^{Cl\otimes L}_V W)$. (This identity holds 
for
$\nabla^{Cl}_V$
so it's trivial to check it follows as well 
for $\nabla^{Cl\otimes L}=
\nabla{Cl}\otimes \mathbbm{1}
+
\mathbbm{1}
\otimes
\nabla^L$
).

Let $\alpha\in\Omega^{0,\cdot}(M,L)$. $\alpha$ is 
locally of the form $\tilde{\alpha}\otimes s$ for $\tilde{\alpha}
\in
C^\infty
(
M,
\Lambda^\cdot T^{(0,1)}M
)$
and
$v\in C^\infty(M,L)$.

And so if $V\in TM$ we have:
$$
L_V\alpha
=
(L_V\tilde{\alpha})\otimes s
+
\tilde{\alpha}\otimes
(L_V s)
$$.

Where we are abusing the overloading the notation of $L_V$.

By Kostant's formula we have that:

$$
\nabla^L_{K^M}=
L_{K^M}
+
2\pi i(\mu, K),\quad
\forall K\in\mathfrak{g}
$$

Then we can express $\mu^M$ in the given coordinates:
$
\mu^M
=
\sum_{i=1}^{dim G}\mu_i V_i
$

and so:
$$
\nabla^L_{\mu^M}s
=
\sum_{i=1}^{dim G}\mu_i \nabla^L_{V_i^M}s
=
\sum_{i=1}^{dim G}\mu_i \left(
L_{V_i^M}s+2\pi i (\mu, V_i)
\right)s
$$

Finally, notice that although
$
\nabla^L_{\mu^M}
$
is not a linear operator because 
$
\nabla^{Cl}_{\mu^M}(\phi \beta)
=
(L_{\mu^M}\phi)\beta 
+
\phi
\nabla_{\mu^M}^{Cl}\beta
$.
 But this means it is an affine operator, so it's a summand of
 the Lie derivative plus some linear operator, $\tilde{A}_M$, so we define:
 $\tilde{\tilde{A}}_M\defequal \tilde{A_M}\otimes\mathbbm{1}$
 
Putting everything together we get:

\begin{align*}
D_T^2
&=
D^2
+T^2
\vert
\mu^M\vert^2
+
iT
(
Dc(\mu^M)
+c(\mu^M)D
)
\\
&=
D^2
+T^2
+T
\left(
ic(e_j)c(\nabla_{e_j}^{TM}\mu^M)
-2i
\sum_{i=1}^{dim G}
(
\mu_i
(2\pi i (\mu,V_i)+\tilde{\tilde{A}}_M)
\right)
-2iT
\left(
\sum_{i=1}^{dim G}
\mu_i
L_{V_i}
\right)
\end{align*}

And so defining $A\defequal
ic(e_j)c(\nabla_{e_j}^{TM}\mu^M)
-2i
\sum_{i=1}^{dim G}
(
\mu_i
(2\pi i (\mu,V_i)+\tilde{\tilde{A}}_M)
$
we are done.

\subsection*{5}

We know from $(0.15)$
that:
$$
\mu^{-1}(0)\subset
Crit(\mathcal{H}))
=
\{
x\in M\vert
\mu^M\vert_x=0
\}
=
\emptyset
$$

On one hand $M$ is compact and
$\vert\mu^M\vert>0$ by hypothesis. This means we can choose $T$
large enough so that $T^2\vert\mu^M\vert^2$ is larger than 
the rest of the terms in $(0.24)$. (Notice some other terms also depend
on $T$, but they depend lineally, so we can dominate them).
This means, for this $T$ sufficiently large we have 
$\langle D_T,D_T\rangle>0$
and so $\ker D_T=\{0\}$.

Since
by definition $ind(D_+)^G
=
\dim
(\ker D_+)^G
-
(\ker D_-)^G$

and both $(\ker D_+)^G$
and $(\ker D_-)^G$
are subspaces of 
$(\ker D_\pm)^G$
we get that their dimension is both $0$.

So $ind(D_+)^G=0$, which is what we wanted.

\end{document}